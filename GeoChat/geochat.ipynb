{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 01:34:57,143] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from time import time\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import html\n",
    "import os\n",
    "gpu_id = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "bounding_box_size = 100\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from garuda.od import ConfusionMatrix\n",
    "from typing import List\n",
    "from garuda.core import obb_iou\n",
    "from dataclasses import dataclass\n",
    "# from my_metrics.metrics import ConfusionMatrix as MyConfusionMatrix\n",
    "\n",
    "from geochat.model.builder import load_pretrained_model\n",
    "from geochat.mm_utils import  get_model_name_from_path\n",
    "from geochat.conversation import conv_templates, Chat\n",
    "\n",
    "from PIL import Image, ImageEnhance\n",
    "from glob import glob\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def escape_markdown(text):\n",
    "    # List of Markdown special characters that need to be escaped\n",
    "    md_chars = ['<', '>']\n",
    "\n",
    "    # Escape each special character\n",
    "    for char in md_chars:\n",
    "        text = text.replace(char, '\\\\' + char)\n",
    "\n",
    "    return text\n",
    "\n",
    "def reverse_escape(text):\n",
    "    md_chars = ['\\\\<', '\\\\>']\n",
    "\n",
    "    for char in md_chars:\n",
    "        text = text.replace(char, char[1:])\n",
    "\n",
    "    return text\n",
    "\n",
    "def bbox_and_angle_to_polygon(x1, y1, x2, y2, a):\n",
    "    # Calculate center coordinates\n",
    "    x_ctr = (x1 + x2) / 2\n",
    "    y_ctr = (y1 + y2) / 2\n",
    "    \n",
    "    # Calculate width and height\n",
    "    w = abs(x2 - x1)\n",
    "    h = abs(y2 - y1)\n",
    "    \n",
    "    # Calculate the angle in radians\n",
    "    angle_rad = math.radians(a)\n",
    "    \n",
    "    # Calculate coordinates of the four corners of the rotated bounding box\n",
    "    cos_a = math.cos(angle_rad)\n",
    "    sin_a = math.sin(angle_rad)\n",
    "    \n",
    "    x1_rot = cos_a * (-w / 2) - sin_a * (-h / 2) + x_ctr\n",
    "    y1_rot = sin_a * (-w / 2) + cos_a * (-h / 2) + y_ctr\n",
    "    \n",
    "    x2_rot = cos_a * (w / 2) - sin_a * (-h / 2) + x_ctr\n",
    "    y2_rot = sin_a * (w / 2) + cos_a * (-h / 2) + y_ctr\n",
    "    \n",
    "    x3_rot = cos_a * (w / 2) - sin_a * (h / 2) + x_ctr\n",
    "    y3_rot = sin_a * (w / 2) + cos_a * (h / 2) + y_ctr\n",
    "    \n",
    "    x4_rot = cos_a * (-w / 2) - sin_a * (h / 2) + x_ctr\n",
    "    y4_rot = sin_a * (-w / 2) + cos_a * (h / 2) + y_ctr\n",
    "    \n",
    "    # Return the polygon coordinates\n",
    "    polygon_coords = np.array((x1_rot, y1_rot, x2_rot, y2_rot, x3_rot, y3_rot, x4_rot, y4_rot))\n",
    "    \n",
    "    return polygon_coords\n",
    "\n",
    "def rotate_bbox(top_right, bottom_left, angle_degrees):\n",
    "    # Convert angle to radians\n",
    "    angle_radians = np.radians(angle_degrees)\n",
    "\n",
    "    # Calculate the center of the rectangle\n",
    "    center = ((top_right[0] + bottom_left[0]) / 2, (top_right[1] + bottom_left[1]) / 2)\n",
    "\n",
    "    # Calculate the width and height of the rectangle\n",
    "    width = top_right[0] - bottom_left[0]\n",
    "    height = top_right[1] - bottom_left[1]\n",
    "\n",
    "    # Create a rotation matrix\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle_degrees, 1)\n",
    "\n",
    "    # Create an array of the rectangle corners\n",
    "    rectangle_points = np.array([[bottom_left[0], bottom_left[1]],\n",
    "                                 [top_right[0], bottom_left[1]],\n",
    "                                 [top_right[0], top_right[1]],\n",
    "                                 [bottom_left[0], top_right[1]]], dtype=np.float32)\n",
    "\n",
    "    # Rotate the rectangle points\n",
    "    rotated_rectangle = cv2.transform(np.array([rectangle_points]), rotation_matrix)[0]\n",
    "\n",
    "    return rotated_rectangle\n",
    "def extract_substrings(string):\n",
    "    # first check if there is no-finished bracket\n",
    "    index = string.rfind('}')\n",
    "    if index != -1:\n",
    "        string = string[:index + 1]\n",
    "\n",
    "    pattern = r'<p>(.*?)\\}(?!<)'\n",
    "    matches = re.findall(pattern, string)\n",
    "    substrings = [match for match in matches]\n",
    "\n",
    "    return substrings\n",
    "\n",
    "\n",
    "def is_overlapping(rect1, rect2):\n",
    "    x1, y1, x2, y2 = rect1\n",
    "    x3, y3, x4, y4 = rect2\n",
    "    return not (x2 < x3 or x1 > x4 or y2 < y3 or y1 > y4)\n",
    "\n",
    "\n",
    "def computeIoU(bbox1, bbox2):\n",
    "    x1, y1, x2, y2 = bbox1\n",
    "    x3, y3, x4, y4 = bbox2\n",
    "    intersection_x1 = max(x1, x3)\n",
    "    intersection_y1 = max(y1, y3)\n",
    "    intersection_x2 = min(x2, x4)\n",
    "    intersection_y2 = min(y2, y4)\n",
    "    intersection_area = max(0, intersection_x2 - intersection_x1 + 1) * max(0, intersection_y2 - intersection_y1 + 1)\n",
    "    bbox1_area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    bbox2_area = (x4 - x3 + 1) * (y4 - y3 + 1)\n",
    "    union_area = bbox1_area + bbox2_area - intersection_area\n",
    "    iou = intersection_area / union_area\n",
    "    return iou\n",
    "\n",
    "\n",
    "def save_tmp_img(visual_img, img_name):\n",
    "    # file_name = \"\".join([str(random.randint(0, 9)) for _ in range(5)]) + \".jpg\"\n",
    "    # file_path = \"/tmp/gradio\" + file_name\n",
    "    file_path = \"/tmp/gradio\" + img_name\n",
    "    visual_img.save(file_path)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def mask2bbox(mask):\n",
    "    if mask is None:\n",
    "        return ''\n",
    "    mask = mask.resize([100, 100], resample=Image.NEAREST)\n",
    "    mask = np.array(mask)[:, :, 0]\n",
    "\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "\n",
    "    if rows.sum():\n",
    "        # Get the top, bottom, left, and right boundaries\n",
    "        rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "        cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "        bbox = '{{<{}><{}><{}><{}>}}'.format(cmin, rmin, cmax, rmax)\n",
    "    else:\n",
    "        bbox = ''\n",
    "\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def escape_markdown(text):\n",
    "    # List of Markdown special characters that need to be escaped\n",
    "    md_chars = ['<', '>']\n",
    "\n",
    "    # Escape each special character\n",
    "    for char in md_chars:\n",
    "        text = text.replace(char, '\\\\' + char)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def reverse_escape(text):\n",
    "    md_chars = ['\\\\<', '\\\\>']\n",
    "\n",
    "    for char in md_chars:\n",
    "        text = text.replace(char, char[1:])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "colors = [\n",
    "    (255, 0, 0),\n",
    "    (0, 255, 0),\n",
    "    (0, 0, 255),\n",
    "    (210, 210, 0),\n",
    "    (255, 0, 255),\n",
    "    (0, 255, 255),\n",
    "    (114, 128, 250),\n",
    "    (0, 165, 255),\n",
    "    (0, 128, 0),\n",
    "    (144, 238, 144),\n",
    "    (238, 238, 175),\n",
    "    (255, 191, 0),\n",
    "    (0, 128, 0),\n",
    "    (226, 43, 138),\n",
    "    (255, 0, 255),\n",
    "    (0, 215, 255),\n",
    "]\n",
    "\n",
    "color_map = {\n",
    "    f\"{color_id}\": f\"#{hex(color[2])[2:].zfill(2)}{hex(color[1])[2:].zfill(2)}{hex(color[0])[2:].zfill(2)}\" for\n",
    "    color_id, color in enumerate(colors)\n",
    "}\n",
    "\n",
    "used_colors = colors\n",
    "\n",
    "\n",
    "def visualize_all_bbox_together(image, generation):\n",
    "    \n",
    "    if image is None:\n",
    "        return None, ''\n",
    "\n",
    "    generation = html.unescape(generation)\n",
    "    image_width, image_height = image.size\n",
    "    image = image.resize([500, int(500 / image_width * image_height)]) # if image width and hight are same then image is of size 500x500\n",
    "    image_width, image_height = image.size\n",
    "\n",
    "    string_list = extract_substrings(generation)\n",
    "    # print(f'String list {string_list}')\n",
    "    if string_list:  # it is grounding or detection\n",
    "        mode = 'all'\n",
    "        entities = defaultdict(list)\n",
    "        i = 0\n",
    "        j = 0\n",
    "        for string in string_list:\n",
    "            try:\n",
    "                obj, string = string.split('</p>')\n",
    "            except ValueError:\n",
    "                print('wrong string: ', string)\n",
    "                continue\n",
    "            if \"}{\" in string:\n",
    "                string=string.replace(\"}{\",\"}<delim>{\")\n",
    "            bbox_list = string.split('<delim>')\n",
    "            # print(f'bbox_list {bbox_list}')\n",
    "            flag = False\n",
    "            for bbox_string in bbox_list:\n",
    "                integers = re.findall(r'-?\\d+', bbox_string)\n",
    "                if len(integers)==4:\n",
    "                    angle=0\n",
    "                else:\n",
    "                    angle=integers[4]\n",
    "                integers=integers[:-1]\n",
    "                \n",
    "                if len(integers) == 4:\n",
    "                    x0, y0, x1, y1 = int(integers[0]), int(integers[1]), int(integers[2]), int(integers[3])\n",
    "                    left = x0 / bounding_box_size * image_width\n",
    "                    bottom = y0 / bounding_box_size * image_height\n",
    "                    right = x1 / bounding_box_size * image_width\n",
    "                    top = y1 / bounding_box_size * image_height\n",
    "\n",
    "                    entities[obj].append([left, bottom, right, top,angle])\n",
    "\n",
    "                    j += 1\n",
    "                    flag = True\n",
    "            if flag:\n",
    "                i += 1\n",
    "    else:\n",
    "        integers = re.findall(r'-?\\d+', generation)\n",
    "        # if len(integers)==4:\n",
    "        angle=0\n",
    "        # else:\n",
    "            # angle=integers[4]\n",
    "        integers=integers[:-1]\n",
    "        if len(integers) == 4:  # it is refer\n",
    "            mode = 'single'\n",
    "\n",
    "            entities = list()\n",
    "            x0, y0, x1, y1 = int(integers[0]), int(integers[1]), int(integers[2]), int(integers[3])\n",
    "            left = x0 / bounding_box_size * image_width\n",
    "            bottom = y0 / bounding_box_size * image_height\n",
    "            right = x1 / bounding_box_size * image_width\n",
    "            top = y1 / bounding_box_size * image_height\n",
    "            entities.append([left, bottom, right, top,angle])\n",
    "        else:\n",
    "            # don't detect any valid bbox to visualize\n",
    "            return None, '', None\n",
    "\n",
    "    if len(entities) == 0:\n",
    "        return None, '', None\n",
    "\n",
    "    if isinstance(image, Image.Image):\n",
    "        image_h = image.height\n",
    "        image_w = image.width\n",
    "        image = np.array(image)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"invalid image format, {type(image)} for {image}\")\n",
    "\n",
    "    indices = list(range(len(entities)))\n",
    "\n",
    "    new_image = image.copy()\n",
    "\n",
    "    previous_bboxes = []\n",
    "    # size of text\n",
    "    text_size = 0.4\n",
    "    # thickness of text\n",
    "    text_line = 1  # int(max(1 * min(image_h, image_w) / 512, 1))\n",
    "    box_line = 2\n",
    "    (c_width, text_height), _ = cv2.getTextSize(\"F\", cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)\n",
    "    base_height = int(text_height * 0.675)\n",
    "    text_offset_original = text_height - base_height\n",
    "    text_spaces = 2\n",
    "\n",
    "\n",
    "    used_colors = colors  # random.sample(colors, k=num_bboxes)\n",
    "\n",
    "    color_id = -1\n",
    "    # print(f'entities {entities}')\n",
    "    for entity_idx, entity_name in enumerate(entities):\n",
    "        if mode == 'single' or mode == 'identify':\n",
    "            bbox_coords = []\n",
    "            bboxes = entity_name\n",
    "            bboxes = [bboxes]\n",
    "        else:\n",
    "            bbox_coords = defaultdict(list)\n",
    "            bboxes = entities[entity_name]\n",
    "        color_id += 1\n",
    "        for bbox_id, (x1_norm, y1_norm, x2_norm, y2_norm,angle) in enumerate(bboxes):\n",
    "            skip_flag = False\n",
    "            orig_x1, orig_y1, orig_x2, orig_y2,angle = int(x1_norm), int(y1_norm), int(x2_norm), int(y2_norm), int(angle)\n",
    "\n",
    "            color = used_colors[entity_idx % len(used_colors)] # tuple(np.random.randint(0, 255, size=3).tolist())\n",
    "            top_right=(orig_x1,orig_y1)\n",
    "            bottom_left=(orig_x2,orig_y2)\n",
    "            angle=angle\n",
    "            rotated_bbox = rotate_bbox(top_right, bottom_left, angle)\n",
    "            # print(f'rotated_bbox {rotated_bbox}')\n",
    "            if mode == 'single' or mode == 'identify':\n",
    "                bbox_coords.append(rotated_bbox/500)\n",
    "            else:\n",
    "                bbox_coords[entity_name].append(rotated_bbox/500)\n",
    "            new_image=cv2.polylines(new_image, [rotated_bbox.astype(np.int32)], isClosed=True, thickness=2, color=color)\n",
    "\n",
    "\n",
    "            if mode == 'all':\n",
    "                l_o, r_o = box_line // 2 + box_line % 2, box_line // 2 + box_line % 2 + 1\n",
    "\n",
    "                x1 = orig_x1 - l_o\n",
    "                y1 = orig_y1 - l_o\n",
    "\n",
    "                if y1 < text_height + text_offset_original + 2 * text_spaces:\n",
    "                    y1 = orig_y1 + r_o + text_height + text_offset_original + 2 * text_spaces\n",
    "                    x1 = orig_x1 + r_o\n",
    "\n",
    "                # add text background\n",
    "                (text_width, text_height), _ = cv2.getTextSize(f\"  {entity_name}\", cv2.FONT_HERSHEY_COMPLEX, text_size,\n",
    "                                                               text_line)\n",
    "                text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2 = x1, y1 - (\n",
    "                            text_height + text_offset_original + 2 * text_spaces), x1 + text_width, y1\n",
    "                \n",
    "                if not skip_flag:\n",
    "                    alpha = 0.5\n",
    "                    for i in range(text_bg_y1, text_bg_y2):\n",
    "                        for j in range(text_bg_x1, text_bg_x2):\n",
    "                            if i < image_h and j < image_w:\n",
    "                                if j < text_bg_x1 + 1.35 * c_width:\n",
    "                                    # original color\n",
    "                                    bg_color = color\n",
    "                                else:\n",
    "                                    # white\n",
    "                                    bg_color = [255, 255, 255]\n",
    "                                new_image[i, j] = (alpha * new_image[i, j] + (1 - alpha) * np.array(bg_color)).astype(\n",
    "                                    np.uint8)\n",
    "\n",
    "                    cv2.putText(\n",
    "                        new_image, f\"  {entity_name}\", (x1, y1 - text_offset_original - 1 * text_spaces),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, text_size, (0, 0, 0), text_line, cv2.LINE_AA\n",
    "                    )\n",
    "\n",
    "                    previous_bboxes.append(\n",
    "                        {'bbox': (text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), 'phrase': entity_name})\n",
    "\n",
    "    if mode == 'all':\n",
    "        def color_iterator(colors):\n",
    "            while True:\n",
    "                for color in colors:\n",
    "                    yield color\n",
    "\n",
    "        color_gen = color_iterator(colors)\n",
    "\n",
    "        # Add colors to phrases and remove <p></p>\n",
    "        def colored_phrases(match):\n",
    "            phrase = match.group(1)\n",
    "            color = next(color_gen)\n",
    "            return f'<span style=\"color:rgb{color}\">{phrase}</span>'\n",
    "\n",
    "        generation = re.sub(r'{<\\d+><\\d+><\\d+><\\d+>}|<delim>', '', generation)\n",
    "        generation_colored = re.sub(r'<p>(.*?)</p>', colored_phrases, generation)\n",
    "    else:\n",
    "        generation_colored = ''\n",
    "\n",
    "    pil_image = Image.fromarray(new_image)\n",
    "    # print(f'bbox_coords {bbox_coords}')\n",
    "    return pil_image, generation_colored, bbox_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class MeanAveragePrecision:\n",
    "    \"\"\"\n",
    "    Mean Average Precision for object detection tasks.\n",
    "\n",
    "    Attributes:\n",
    "        map50_95 (float): Mean Average Precision (mAP) calculated over IoU thresholds\n",
    "            ranging from `0.50` to `0.95` with a step size of `0.05`.\n",
    "        map50 (float): Mean Average Precision (mAP) calculated specifically at\n",
    "            an IoU threshold of `0.50`.\n",
    "        map75 (float): Mean Average Precision (mAP) calculated specifically at\n",
    "            an IoU threshold of `0.75`.\n",
    "        per_class_ap50_95 (np.ndarray): Average Precision (AP) values calculated over\n",
    "            IoU thresholds ranging from `0.50` to `0.95` with a step size of `0.05`,\n",
    "            provided for each individual class.\n",
    "    \"\"\"\n",
    "\n",
    "    map50_95: float\n",
    "    map50: float\n",
    "    map75: float\n",
    "    per_class_ap50_95: np.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def from_tensors(\n",
    "        cls,\n",
    "        predictions: List[np.ndarray],\n",
    "        targets: List[np.ndarray],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculate Mean Average Precision based on predicted and ground-truth\n",
    "            detections at different threshold.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[np.ndarray]): Each element of the list describes\n",
    "                a single image and has `shape = (M, 6)` where `M` is\n",
    "                the number of detected objects. Each row is expected to be\n",
    "                in `(x_min, y_min, x_max, y_max, class, conf)` format.\n",
    "            targets (List[np.ndarray]): Each element of the list describes a single\n",
    "                image and has `shape = (N, 5)` where `N` is the\n",
    "                number of ground-truth objects. Each row is expected to be in\n",
    "                `(x_min, y_min, x_max, y_max, class)` format.\n",
    "        Returns:\n",
    "            MeanAveragePrecision: New instance of MeanAveragePrecision.\n",
    "\n",
    "        Example:\n",
    "            ```python\n",
    "            import supervision as sv\n",
    "            import numpy as np\n",
    "\n",
    "            targets = (\n",
    "                [\n",
    "                    np.array(\n",
    "                        [\n",
    "                            [0.0, 0.0, 3.0, 3.0, 1],\n",
    "                            [2.0, 2.0, 5.0, 5.0, 1],\n",
    "                            [6.0, 1.0, 8.0, 3.0, 2],\n",
    "                        ]\n",
    "                    ),\n",
    "                    np.array([[1.0, 1.0, 2.0, 2.0, 2]]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            predictions = [\n",
    "                np.array(\n",
    "                    [\n",
    "                        [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n",
    "                        [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n",
    "                        [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n",
    "                        [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n",
    "                    ]\n",
    "                ),\n",
    "                np.array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n",
    "            ]\n",
    "\n",
    "            mean_average_precision = sv.MeanAveragePrecision.from_tensors(\n",
    "                predictions=predictions,\n",
    "                targets=targets,\n",
    "            )\n",
    "\n",
    "            print(mean_average_precision.map50_95)\n",
    "            # 0.6649\n",
    "            ```\n",
    "        \"\"\"\n",
    "        # validate_input_tensors(predictions, targets)\n",
    "        iou_thresholds = np.linspace(0.5, 0.95, 10)\n",
    "        stats = []\n",
    "\n",
    "        # Gather matching stats for predictions and targets\n",
    "        for true_objs, predicted_objs in zip(targets, predictions):\n",
    "            if predicted_objs.shape[0] == 0:\n",
    "                if true_objs.shape[0]:\n",
    "                    stats.append(\n",
    "                        (\n",
    "                            np.zeros((0, iou_thresholds.size), dtype=bool),\n",
    "                            *np.zeros((2, 0)),\n",
    "                            true_objs[:, 0], # index 0 is class\n",
    "                        )\n",
    "                    )\n",
    "                continue\n",
    "\n",
    "            if true_objs.shape[0]:\n",
    "                matches = cls._match_detection_batch(\n",
    "                    predicted_objs, true_objs, iou_thresholds\n",
    "                )\n",
    "                stats.append(\n",
    "                    (\n",
    "                        matches,\n",
    "                        predicted_objs[:, -1],\n",
    "                        predicted_objs[:, 0],\n",
    "                        true_objs[:, 0],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Compute average precisions if any matches exist\n",
    "        if stats:\n",
    "            concatenated_stats = [np.concatenate(items, 0) for items in zip(*stats)]\n",
    "            average_precisions = cls._average_precisions_per_class(*concatenated_stats)\n",
    "            map50 = average_precisions[:, 0].mean()\n",
    "            map75 = average_precisions[:, 5].mean()\n",
    "            map50_95 = average_precisions.mean()\n",
    "        else:\n",
    "            map50, map75, map50_95 = 0, 0, 0\n",
    "            average_precisions = []\n",
    "\n",
    "        return cls(\n",
    "            map50_95=map50_95,\n",
    "            map50=map50,\n",
    "            map75=map75,\n",
    "            per_class_ap50_95=average_precisions,\n",
    "        )\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _match_detection_batch(\n",
    "        predictions: np.ndarray, targets: np.ndarray, iou_thresholds: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Match predictions with target labels based on IoU levels.\n",
    "\n",
    "        Args:\n",
    "            predictions (np.ndarray): Batch prediction. Describes a single image and\n",
    "                has `shape = (M, 6)` where `M` is the number of detected objects.\n",
    "                Each row is expected to be in\n",
    "                `(x_min, y_min, x_max, y_max, class, conf)` format.\n",
    "            targets (np.ndarray): Batch target labels. Describes a single image and\n",
    "                has `shape = (N, 5)` where `N` is the number of ground-truth objects.\n",
    "                Each row is expected to be in\n",
    "                `(x_min, y_min, x_max, y_max, class)` format.\n",
    "            iou_thresholds (np.ndarray): Array contains different IoU thresholds.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Matched prediction with target labels result.\n",
    "        \"\"\"\n",
    "        num_predictions, num_iou_levels = predictions.shape[0], iou_thresholds.shape[0]\n",
    "        correct = np.zeros((num_predictions, num_iou_levels), dtype=bool)\n",
    "        iou = obb_iou(targets[:, 1:9].reshape(-1,4,2), predictions[:, 1:9].reshape(-1,4,2))\n",
    "        correct_class = targets[:, 0:1] == predictions[:, 0]\n",
    "\n",
    "        for i, iou_level in enumerate(iou_thresholds):\n",
    "            matched_indices = np.where((iou >= iou_level) & correct_class)\n",
    "\n",
    "            if matched_indices[0].shape[0]:\n",
    "                combined_indices = np.stack(matched_indices, axis=1)\n",
    "                iou_values = iou[matched_indices][:, None]\n",
    "                matches = np.hstack([combined_indices, iou_values])\n",
    "\n",
    "                if matched_indices[0].shape[0] > 1:\n",
    "                    matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                    matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "                    matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "\n",
    "                correct[matches[:, 1].astype(int), i] = True\n",
    "\n",
    "        return correct\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_average_precision(recall: np.ndarray, precision: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute the average precision using 101-point interpolation (COCO), given\n",
    "            the recall and precision curves.\n",
    "\n",
    "        Args:\n",
    "            recall (np.ndarray): The recall curve.\n",
    "            precision (np.ndarray): The precision curve.\n",
    "\n",
    "        Returns:\n",
    "            float: Average precision.\n",
    "        \"\"\"\n",
    "        extended_recall = np.concatenate(([0.0], recall, [1.0]))\n",
    "        extended_precision = np.concatenate(([1.0], precision, [0.0]))\n",
    "        max_accumulated_precision = np.flip(\n",
    "            np.maximum.accumulate(np.flip(extended_precision))\n",
    "        )\n",
    "        interpolated_recall_levels = np.linspace(0, 1, 101)\n",
    "        interpolated_precision = np.interp(\n",
    "            interpolated_recall_levels, extended_recall, max_accumulated_precision\n",
    "        )\n",
    "        average_precision = np.trapz(interpolated_precision, interpolated_recall_levels)\n",
    "        return average_precision\n",
    "\n",
    "    @staticmethod\n",
    "    def _average_precisions_per_class(\n",
    "        matches: np.ndarray,\n",
    "        prediction_confidence: np.ndarray,\n",
    "        prediction_class_ids: np.ndarray,\n",
    "        true_class_ids: np.ndarray,\n",
    "        eps: float = 1e-16,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the average precision, given the recall and precision curves.\n",
    "        Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "\n",
    "        Args:\n",
    "            matches (np.ndarray): True positives.\n",
    "            prediction_confidence (np.ndarray): Objectness value from 0-1.\n",
    "            prediction_class_ids (np.ndarray): Predicted object classes.\n",
    "            true_class_ids (np.ndarray): True object classes.\n",
    "            eps (float): Small value to prevent division by zero.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Average precision for different IoU levels.\n",
    "        \"\"\"\n",
    "        sorted_indices = np.argsort(-prediction_confidence)\n",
    "        matches = matches[sorted_indices]\n",
    "        prediction_class_ids = prediction_class_ids[sorted_indices]\n",
    "\n",
    "        unique_classes, class_counts = np.unique(true_class_ids, return_counts=True)\n",
    "        num_classes = unique_classes.shape[0]\n",
    "\n",
    "        average_precisions = np.zeros((num_classes, matches.shape[1]))\n",
    "\n",
    "        for class_idx, class_id in enumerate(unique_classes):\n",
    "            is_class = prediction_class_ids == class_id\n",
    "            total_true = class_counts[class_idx]\n",
    "            total_prediction = is_class.sum()\n",
    "\n",
    "            if total_prediction == 0 or total_true == 0:\n",
    "                continue\n",
    "\n",
    "            false_positives = (1 - matches[is_class]).cumsum(0)\n",
    "            true_positives = matches[is_class].cumsum(0)\n",
    "            recall = true_positives / (total_true + eps)\n",
    "            precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "            for iou_level_idx in range(matches.shape[1]):\n",
    "                average_precisions[class_idx, iou_level_idx] = (\n",
    "                    MeanAveragePrecision.compute_average_precision(\n",
    "                        recall[:, iou_level_idx], precision[:, iou_level_idx]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return average_precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_predicted_target_bbox(chat, CONV_VISION, gr_imgs_path, gr_labels, type, user_message):    \n",
    "    image_path_with_detected_objects = []\n",
    "\n",
    "    target_results = []\n",
    "    predicted_results = []\n",
    "\n",
    "    total_execution_time = 0\n",
    "    for img_path in sorted(glob(gr_imgs_path)):\n",
    "        gr_img = Image.open(img_path).convert('RGB')\n",
    "        img_list = []\n",
    "        chat_state = CONV_VISION.copy()\n",
    "        llm_message = chat.upload_img(gr_img, chat_state, img_list = img_list)\n",
    "        # print(llm_message)\n",
    "        chat.ask(user_message, chat_state) # ask the question grounding, refer, expression, scene classification etc\n",
    "\n",
    "        if len(img_list) > 0:\n",
    "            if not isinstance(img_list[0], torch.Tensor):\n",
    "                chat.encode_img(img_list)\n",
    "        time_start = time()     \n",
    "        streamer = chat.stream_answer(conv=chat_state,\n",
    "                                        img_list=img_list,\n",
    "                                        temperature=0.5,\n",
    "                                        max_new_tokens=500,\n",
    "                                        max_length=2000)\n",
    "        time_end = time()\n",
    "        total_execution_time += time_end - time_start\n",
    "        # print(f'Streamer {streamer}')    \n",
    "        output = ''\n",
    "        for new_output in streamer:\n",
    "            # print(new_output)\n",
    "            output=output+new_output\n",
    "        # print(output)\n",
    "\n",
    "        # output = escape_markdown(output)\n",
    "        chat_state.messages[-1][1] = '</s>'\n",
    "\n",
    "        # output = reverse_escape(output)\n",
    "        # print(output)\n",
    "        visual_img, generation_color, bbox = visualize_all_bbox_together(gr_img, output) # None, dictionary, list\n",
    "        # print(visual_img)\n",
    "        img_name = os.path.basename(img_path)\n",
    "        if visual_img is not None:\n",
    "            file_path = save_tmp_img(visual_img, img_name)\n",
    "            image_path_with_detected_objects.append(file_path)\n",
    "        # print(generation_color)\n",
    "\n",
    "        if bbox is None:\n",
    "            # predicted_results.append(np.zeros((1, 8)))\n",
    "            predicted_results.append(np.array([])) # no bbox detected\n",
    "        elif isinstance(bbox, dict): # grounding return bboxes in dict type\n",
    "            tmp = []\n",
    "            for key, values in bbox.items():\n",
    "                for value in values:\n",
    "                    tmp.append(np.array(value).reshape(-1))\n",
    "            predicted_results.append(np.array(tmp))\n",
    "        else: # [refer] return bboxes in list type\n",
    "            tmp = []\n",
    "            for value in bbox:\n",
    "                    tmp.append(np.array(value).reshape(-1))\n",
    "                    \n",
    "            # save the predictions in txt file - folder name (image_type_prompt), file name (image_name)\n",
    "            # np.savetxt(f'{type}/{user_message}/{img_name[:-4]}.txt',np.array(tmp))\n",
    "            predicted_results.append(np.array(tmp))\n",
    "        if '.tif' in img_name:\n",
    "            target_path = os.path.join(gr_labels, img_name.replace('.tif', '.txt'))\n",
    "        else:\n",
    "            target_path = os.path.join(gr_labels, img_name.replace('.png', '.txt'))\n",
    "        target_results.append(np.loadtxt(target_path, ndmin=2))\n",
    "\n",
    "    return predicted_results, target_results, image_path_with_detected_objects, total_execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_class_confidence(predicted_results):\n",
    "    new_predicted_results = []\n",
    "    for res in predicted_results:\n",
    "        if len(res):\n",
    "            res = np.hstack([np.zeros((len(res),1)), res, np.ones((len(res),1))], dtype=np.float32) # add class label 0 at index 0 and confidence score 1 at last index\n",
    "            new_predicted_results.append(res)\n",
    "        else:\n",
    "            res = np.zeros((1, 10))\n",
    "            res[:, 0] = 1\n",
    "            res[:, -1] = 1\n",
    "            new_predicted_results.append(res.astype(np.float32))\n",
    "    return new_predicted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def modify_class(target_results):\n",
    "    new_target_results = []\n",
    "    for res in target_results:\n",
    "        res[:,0] = 0 # convert class labels to 0\n",
    "        res = res.astype(np.float32)\n",
    "        new_target_results.append(res)\n",
    "    return new_target_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_confusion_matrix(new_predicted_results, new_target_results):\n",
    "    cm_predicted_results = []\n",
    "    for res in new_predicted_results:\n",
    "        res[:,1:9] = res[:,1:9]*500\n",
    "        cm_predicted_results.append(res)\n",
    "\n",
    "\n",
    "    cm_target_results = []\n",
    "    for res in new_target_results:\n",
    "        res[:,0] = 0 # convert class labels to 0\n",
    "        res[:,1:9] = res[:,1:9]*500\n",
    "        res = res.astype(np.float32)\n",
    "        cm_target_results.append(res)\n",
    "\n",
    "    classes, conf_threshold, iou_threshold = ['brick_kilns'], 0.25, 0.5\n",
    "    cm = ConfusionMatrix.from_obb_tensors(cm_predicted_results, cm_target_results, classes, conf_threshold, iou_threshold)\n",
    "    # cm = MyConfusionMatrix.from_tensors(cm_predicted_results, cm_target_results, classes, conf_threshold, iou_threshold)\n",
    "    df = pd.DataFrame(cm.matrix, columns = ['predicted kilns','predicted_bg'], index=['true kilns','true_bg'])\n",
    "    print(f'conf_threshold = {conf_threshold}, iou_threshold = {iou_threshold}')\n",
    "    # print(cm.summary)\n",
    "    # print(df.to_markdown())\n",
    "    return cm, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_precision_recall(confusion_matrix, new_predicted_results, new_target_results):\n",
    "    tp = confusion_matrix.loc['true kilns']['predicted kilns']\n",
    "    predicted_positives = 0\n",
    "    for res in new_predicted_results:\n",
    "        predicted_positives += np.where(res[:,0] == 1, 0, 1).sum() # class = 1 means background class\n",
    "    \n",
    "    ground_truth = 0\n",
    "    for res in new_target_results:\n",
    "        ground_truth += np.where(res[:,0] == 0, 1, 0).sum() # class = 0 means brick kiln class\n",
    "\n",
    "    precision = tp / predicted_positives\n",
    "    recall = tp/ground_truth\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_results(user_message, gr_imgs_path, new_predicted_results, new_target_results, iou_threshold, image_path_with_detected_objects, region, plot=False):\n",
    "    if not plot:\n",
    "        return\n",
    "    # n = len(image_path_with_detected_objects)\n",
    "    gr_img_path = sorted(glob(gr_imgs_path))\n",
    "    n = len(gr_img_path)\n",
    "    cols = 5\n",
    "    rows = math.ceil(n / cols)\n",
    "    fig, ax = plt.subplots(nrows = rows, ncols = cols ,figsize=(cols*8, rows*8))\n",
    "    ax = ax.flatten()\n",
    "    for i in range(n):\n",
    "        # img = Image.open(image_path_with_detected_objects[i]).convert('RGB') # predicted image\n",
    "        img = Image.open(gr_img_path[i]).convert('RGB') # planet image\n",
    "        w, h = img.size\n",
    "        ax[i].imshow(img) \n",
    "        for bbox in new_target_results[i]:\n",
    "            classvalue, x1, y1, x2, y2, x3, y3, x4, y4 = bbox*w \n",
    "            ax[i].plot([x1, x2, x3, x4, x1], [y1, y2, y3, y4, y1], color = 'green', linewidth = 8)\n",
    "        for bbox in new_predicted_results[i]:\n",
    "            classvalue, x1, y1, x2, y2, x3, y3, x4, y4, conf = bbox*w \n",
    "            ax[i].plot([x1, x2, x3, x4, x1], [y1, y2, y3, y4, y1], color = 'red', linewidth = 8)\n",
    "        ax[i].set_axis_off()\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(n, len(ax)):\n",
    "        ax[j].axis('off')\n",
    "    # location = region.split('/data/')[1]\n",
    "    # location = location.replace('/','_')\n",
    "    fig.suptitle(f'{user_message}')\n",
    "    # plt.savefig(f'geochat_output_refer_{location}_planet.png')\n",
    "    region = region.replace('/','_')\n",
    "    plt.savefig(f'geochat_output_refer_{region}.png')\n",
    "    # plt.close() # will not display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geochat-7B\n",
      "Loading GeoChat......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0d69b3f3e846f7a823ee01a3c21228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = 'MBZUAI/geochat-7B'\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "print(model_name)\n",
    "device = 'cuda:{}'.format(0)\n",
    "# set device_map = None to use single GPU, otherwise 'auto' to load model in all GPUs (auto will do sharding it will load layers, weights in different GPUs for better memory efficiency).\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name, device_map = None, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model.eval()\n",
    "chat = Chat(model.to(device), image_processor,tokenizer, device=device)\n",
    "CONV_VISION = conv_templates['llava_v1'].copy()\n",
    "# chat_state = CONV_VISION.copy()\n",
    "# chat_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'default': Conversation(system=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\", roles=('Human', 'Assistant'), messages=(('Human', 'What are the key differences between renewable and non-renewable energy sources?'), ('Assistant', 'Renewable energy sources are those that can be replenished naturally in a relatively short amount of time, such as solar, wind, hydro, geothermal, and biomass. Non-renewable energy sources, on the other hand, are finite and will eventually be depleted, such as coal, oil, and natural gas. Here are some key differences between renewable and non-renewable energy sources:\\n1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable energy sources are finite and will eventually run out.\\n2. Environmental impact: Renewable energy sources have a much lower environmental impact than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, and other negative effects.\\n3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically have lower operational costs than non-renewable sources.\\n4. Reliability: Renewable energy sources are often more reliable and can be used in more remote locations than non-renewable sources.\\n5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different situations and needs, while non-renewable sources are more rigid and inflexible.\\n6. Sustainability: Renewable energy sources are more sustainable over the long term, while non-renewable sources are not, and their depletion can lead to economic and social instability.\\n')), offset=2, sep_style=<SeparatorStyle.SINGLE: 1>, sep='###', sep2=None, version='Unknown', skip_next=False),\n",
       " 'v0': Conversation(system=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\", roles=('Human', 'Assistant'), messages=(('Human', 'What are the key differences between renewable and non-renewable energy sources?'), ('Assistant', 'Renewable energy sources are those that can be replenished naturally in a relatively short amount of time, such as solar, wind, hydro, geothermal, and biomass. Non-renewable energy sources, on the other hand, are finite and will eventually be depleted, such as coal, oil, and natural gas. Here are some key differences between renewable and non-renewable energy sources:\\n1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable energy sources are finite and will eventually run out.\\n2. Environmental impact: Renewable energy sources have a much lower environmental impact than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, and other negative effects.\\n3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically have lower operational costs than non-renewable sources.\\n4. Reliability: Renewable energy sources are often more reliable and can be used in more remote locations than non-renewable sources.\\n5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different situations and needs, while non-renewable sources are more rigid and inflexible.\\n6. Sustainability: Renewable energy sources are more sustainable over the long term, while non-renewable sources are not, and their depletion can lead to economic and social instability.\\n')), offset=2, sep_style=<SeparatorStyle.SINGLE: 1>, sep='###', sep2=None, version='Unknown', skip_next=False),\n",
       " 'v1': Conversation(system=\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", roles=('USER', 'ASSISTANT'), messages=(), offset=0, sep_style=<SeparatorStyle.TWO: 2>, sep=' ', sep2='</s>', version='v1', skip_next=False),\n",
       " 'vicuna_v1': Conversation(system=\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", roles=('USER', 'ASSISTANT'), messages=(), offset=0, sep_style=<SeparatorStyle.TWO: 2>, sep=' ', sep2='</s>', version='v1', skip_next=False),\n",
       " 'llama_2': Conversation(system=\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", roles=('USER', 'ASSISTANT'), messages=(), offset=0, sep_style=<SeparatorStyle.LLAMA_2: 5>, sep='<s>', sep2='</s>', version='llama_v2', skip_next=False),\n",
       " 'plain': Conversation(system='', roles=('', ''), messages=(), offset=0, sep_style=<SeparatorStyle.PLAIN: 4>, sep='\\n', sep2=None, version='Unknown', skip_next=False),\n",
       " 'v0_plain': Conversation(system='', roles=('', ''), messages=(), offset=0, sep_style=<SeparatorStyle.PLAIN: 4>, sep='\\n', sep2=None, version='Unknown', skip_next=False),\n",
       " 'llava_v0': Conversation(system=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\", roles=('Human', 'Assistant'), messages=(), offset=0, sep_style=<SeparatorStyle.SINGLE: 1>, sep='###', sep2=None, version='Unknown', skip_next=False),\n",
       " 'v0_mmtag': Conversation(system='A chat between a curious user and an artificial intelligence assistant. The assistant is able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.The visual content will be provided with the following format: <Image>visual content</Image>.', roles=('Human', 'Assistant'), messages=(), offset=0, sep_style=<SeparatorStyle.SINGLE: 1>, sep='###', sep2=None, version='v0_mmtag', skip_next=False),\n",
       " 'llava_v1': Conversation(system=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\", roles=('USER', 'ASSISTANT'), messages=(), offset=0, sep_style=<SeparatorStyle.TWO: 2>, sep=' ', sep2='</s>', version='v1', skip_next=False),\n",
       " 'v1_mmtag': Conversation(system='A chat between a curious user and an artificial intelligence assistant. The assistant is able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.The visual content will be provided with the following format: <Image>visual content</Image>.', roles=('USER', 'ASSISTANT'), messages=(), offset=0, sep_style=<SeparatorStyle.TWO: 2>, sep=' ', sep2='</s>', version='v1_mmtag', skip_next=False),\n",
       " 'llava_llama_2': Conversation(system='You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.', roles=('USER', 'ASSISTANT'), messages=(), offset=0, sep_style=<SeparatorStyle.LLAMA_2: 5>, sep='<s>', sep2='</s>', version='llama_v2', skip_next=False),\n",
       " 'mpt': Conversation(system='<|im_start|>system\\nA conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.', roles=('<|im_start|>user\\n', '<|im_start|>assistant\\n'), messages=(), offset=0, sep_style=<SeparatorStyle.MPT: 3>, sep='<|im_end|>', sep2=None, version='mpt', skip_next=False)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# user_message = 'Give the bounding box coordinates of all the brick kilns present in the image separated by newline'\n",
    "# user_message = 'Draw bounding box around the brick kiln with chimney in the image'\n",
    "\n",
    "# Visual question answering - given the image and a question, it generates the answer.\n",
    "\n",
    "# Scene classification - given the image, it generates the scene category.\n",
    "# user_message = 'Classify the image in one word. The classes are Church, School, Bareland, Beach, Forest'\n",
    "\n",
    "# Region-level caption - given the bounding box on the image, it generates brief description about the object\n",
    "# user_message = '[identify] What is this object?'\n",
    "\n",
    "# Grounded description - describe the object and give the bounding box.\n",
    "# user_message = 'describe the image in detail'\n",
    "\n",
    "# Refering expressions - refer to the object by providing some attributes : large, top, close etc, will produce the bounding box\n",
    "user_message1 = '[refer] Where is the brick kiln with chimney in the image? Give its bounding box'\n",
    "user_message2 = '[refer] Where are the fields, factories, roads and brick kilns in the image? Give its oriented bounding box'\n",
    "user_message3= '[refer] Where are the fields, factories and roads in the image? Give its oriented bounding box'\n",
    "user_message4 = '[refer] Where are the fields and factories in the image? Give its oriented bounding box'\n",
    "user_message5 = '[refer] Where are the factories in the image? Give its oriented bounding box'\n",
    "user_message6 = '[refer] Where are the fields in the image? Give its oriented bounding box'\n",
    "user_message7 ='[refer] Where are the chimneys in the image? Give its oriented bounding box'\n",
    "user_message8 = '[refer] Where are the factories and roads in the image? Give its oriented bounding box'\n",
    "user_message9 = '[refer] Where are the fields and roads in the image? Give its oriented bounding box'\n",
    "user_message10 = '[refer] Where are the roads in the image? Give its oriented bounding box'\n",
    "user_message11 = '[refer] Where is the chimneys and brick kilns in the image? Give its oriented bounding box'\n",
    "\n",
    "\n",
    "user_messages = [\n",
    "    user_message1, \n",
    "    # user_message2, \n",
    "    # user_message3, \n",
    "    # user_message4,\n",
    "    # user_message5,\n",
    "    # user_message6,\n",
    "    # user_message7,\n",
    "    # user_message8,\n",
    "    # user_message9,\n",
    "    # user_message10,\n",
    "    # user_message11\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing without patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data'\n",
    "\n",
    "regions = [\n",
    "            # f'{base_path}/lucknow_airshed_most_15/images', \n",
    "        #    '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/swinir_data/lucknow_airshed_most_15/images',\n",
    "        #    '/home/shataxi.dubey/shataxi_work/vlm_on_planet/lucknow_high_resolution_zoom17',\n",
    "        #    '/home/shataxi.dubey/shataxi_work/vlm_on_planet/lucknow_high_resolution_zoom18',\n",
    "            # '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data/uttar_pradesh_most_15/images/',\n",
    "        #     f'{base_path}/uttar_pradesh_most_15/swinir_images',\n",
    "        #    '/home/shataxi.dubey/shataxi_work/vlm_on_planet/uttar_pradesh_high_resolution_zoom17',\n",
    "        #    '/home/shataxi.dubey/shataxi_work/vlm_on_planet/uttar_pradesh_high_resolution_zoom18',\n",
    "        #    '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data/west_bengal_most_15/images/',\n",
    "        #    f'{base_path}/west_bengal_most_15/swinir_images',\n",
    "        #    '/home/shataxi.dubey/shataxi_work/vlm_on_planet/west_bengal_high_resolution_zoom17',\n",
    "        #    '/home/shataxi.dubey/shataxi_work/vlm_on_planet/west_bengal_high_resolution_zoom18'\n",
    "        # '/home/shataxi.dubey/shataxi_work/vlm_on_planet/lucknow_kilns_zoom19',\n",
    "        # '/home/shataxi.dubey/shataxi_work/vlm_on_planet/uttar_pradesh_kilns_zoom19',\n",
    "        # '/home/shataxi.dubey/shataxi_work/vlm_on_planet/west_bengal_kilns_zoom19'\n",
    "        '/home/shataxi.dubey/shataxi_work/vlm_on_planet/gms/lucknow_small_600_sq_km/kiln_images/test/images'\n",
    "        ]\n",
    "\n",
    "locations = [\n",
    "            #  'lucknow_airshed_most_15', \n",
    "            #  'lucknow_airshed_most_15', \n",
    "            #  'lucknow_airshed_most_15', \n",
    "            #  'lucknow_airshed_most_15',\n",
    "            #  'uttar_pradesh_most_15',\n",
    "            #  'uttar_pradesh_most_15',\n",
    "            #  'uttar_pradesh_most_15', \n",
    "            #  'uttar_pradesh_most_15', \n",
    "            #  'west_bengal_most_15',\n",
    "            #  'west_bengal_most_15', \n",
    "            #  'west_bengal_most_15', \n",
    "            #  'west_bengal_most_15',\n",
    "            'lucknow_gms_kiln_images'\n",
    "             ]\n",
    "\n",
    "type = ['lucknow_gms_kiln_images']\n",
    "\n",
    "for user_message in user_messages:\n",
    "    for region, location in zip(regions, locations):\n",
    "        print(f'region: {region}, user_message: {user_message}')\n",
    "        gr_imgs_path = region+'/*'\n",
    "        # gr_labels = f'/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data/{location}/labels' # labels related to planet imagery\n",
    "        gr_labels = f'/home/shataxi.dubey/shataxi_work/vlm_on_planet/gms/lucknow_small_600_sq_km/kiln_images/test/labels'\n",
    "        \n",
    "        predicted_results, target_results, image_path_with_detected_objects, total_execution_time = get_predicted_target_bbox(chat, CONV_VISION, gr_imgs_path, gr_labels, type, user_message)\n",
    "        new_predicted_results = add_class_confidence(predicted_results)\n",
    "        new_target_results = modify_class(target_results)\n",
    "        plot_results(user_message, gr_imgs_path, new_predicted_results, new_target_results, 0.1, image_path_with_detected_objects, region, True)\n",
    "        cm, df = calculate_confusion_matrix(new_predicted_results, new_target_results)\n",
    "        # print(f'Precision: {cm.precision}, Recall: {cm.recall}, F1 Score: {cm.f1_score}')\n",
    "        precision, recall, f1_score = calculate_precision_recall(df, new_predicted_results, new_target_results)\n",
    "        print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}')\n",
    "        print(f'Total execution time(s): {total_execution_time}')\n",
    "        display(df)\n",
    "        \n",
    "        print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the image in patch of size 320x320\n",
    "because 320 divides 640, 2560 and 5120\n",
    "\n",
    "\n",
    "Planet, Zoom 17, Zoom 18\n",
    "\n",
    "Overlap of pixels\n",
    "\n",
    "89/4.77, 89/1.1925, 89/0.59625 = (20, 75, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def patch_boxes(image_height , image_width, patch_size, overlap):\n",
    "    slice_bboxes = []\n",
    "    offsets = []\n",
    "    y_max = y_min = 0\n",
    "\n",
    "    while y_max < image_height:\n",
    "        x_min = x_max = 0\n",
    "        y_max = y_min + patch_size\n",
    "        while x_max < image_width:\n",
    "            x_max = x_min + patch_size\n",
    "            if y_max > image_height or x_max > image_width:\n",
    "                xmax = min(image_width, x_max)\n",
    "                ymax = min(image_height, y_max)\n",
    "                xmin = max(0, xmax - patch_size)\n",
    "                ymin = max(0, ymax - patch_size)\n",
    "                slice_bboxes.append([xmin, ymin, xmax, ymax])\n",
    "                offsets.append([xmin, ymin])\n",
    "            else:\n",
    "                slice_bboxes.append([x_min, y_min, x_max, y_max])\n",
    "                offsets.append([x_min, y_min])\n",
    "            x_min = x_max - overlap\n",
    "        y_min = y_max - overlap\n",
    "    return slice_bboxes, offsets\n",
    "\n",
    "\n",
    "\n",
    "def get_predicted_target_bbox_patch(chat, CONV_VISION, gr_imgs_path, patch_size, overlap, gr_labels, type, user_message):    \n",
    "    image_path_with_detected_objects = []\n",
    "\n",
    "    target_results = []\n",
    "    predicted_results = []\n",
    "\n",
    "    total_execution_time = 0\n",
    "    for img_id, img_path in enumerate(glob(gr_imgs_path)):\n",
    "        gr_img = Image.open(img_path).convert('RGB')\n",
    "        # orig_img = Image.open(img_path).convert('RGB')\n",
    "        # gr_img = ImageEnhance.Contrast(orig_img) \n",
    "        # gr_img = gr_img.enhance(2)\n",
    "        patches, offsets = patch_boxes(gr_img.size[0], gr_img.size[1], patch_size, overlap)\n",
    "        tmp = []\n",
    "        num_patches = len(patches)\n",
    "        # fig, ax = plt.subplots(nrows = num_patches, ncols = 1, figsize=(50,50)) # visualise plot to debug\n",
    "        # ax = ax.flatten()\n",
    "        for i, patch_offset in enumerate(zip(patches, offsets)):\n",
    "            patch, offset = patch_offset\n",
    "            patch = gr_img.crop(patch)\n",
    "            # ax[i].imshow(patch) # visualise plot to debug\n",
    "            img_list = []\n",
    "            chat_state = CONV_VISION.copy()\n",
    "            llm_message = chat.upload_img(patch, chat_state, img_list = img_list)\n",
    "            # print(llm_message)\n",
    "            chat.ask(user_message, chat_state) # ask the question grounding, refer, expression, scene classification etc\n",
    "\n",
    "            if len(img_list) > 0:\n",
    "                if not isinstance(img_list[0], torch.Tensor):\n",
    "                    chat.encode_img(img_list)\n",
    "            time_start = time()     \n",
    "            streamer = chat.stream_answer(conv=chat_state,\n",
    "                                            img_list=img_list,\n",
    "                                            temperature=0.5,\n",
    "                                            max_new_tokens=500,\n",
    "                                            max_length=2000)\n",
    "            time_end = time()\n",
    "            total_execution_time += time_end - time_start\n",
    "            output = ''\n",
    "            for new_output in streamer:\n",
    "                output=output+new_output\n",
    "\n",
    "\n",
    "            chat_state.messages[-1][1] = '</s>'\n",
    "\n",
    "\n",
    "            visual_img, generation_color, bbox = visualize_all_bbox_together(patch, output) # None, dictionary, list\n",
    "            img_name = os.path.basename(img_path)\n",
    "            if visual_img is not None:\n",
    "                file_path = save_tmp_img(visual_img, img_name)\n",
    "                image_path_with_detected_objects.append(file_path)\n",
    "\n",
    "            if bbox is None:\n",
    "                pass\n",
    "            elif isinstance(bbox, dict): # grounding return bboxes in dict type\n",
    "                for key, values in bbox.items():\n",
    "                    for value in values:\n",
    "                        # x = np.array(value).reshape(-1)\n",
    "                        # ax[i].plot(x[[0,2,4,6,0]]*patch_size,x[[1,3,5,7,1]]*patch_size, color = 'red') #debug\n",
    "                        value = (value*patch_size + offset)/gr_img.size[0]\n",
    "                        value = np.array(value).reshape(-1)\n",
    "                        tmp.append(value)\n",
    "            else: # [refer] return bboxes in list type\n",
    "                for value in bbox:\n",
    "                        # x = np.array(value).reshape(-1)\n",
    "                        # ax[i].plot(x[[0,2,4,6,0]]*patch_size,x[[1,3,5,7,1]]*patch_size, color = 'red') #debug\n",
    "                        value = (value*patch_size + offset)/gr_img.size[0]\n",
    "                        value = np.array(value).reshape(-1)\n",
    "                        tmp.append(value)\n",
    "                \n",
    "        # save the predictions in txt file - folder name (image_type_prompt), file name (image_name)\n",
    "        np.savetxt(f'{type}/{user_message}/{img_name[:-4]}.txt',np.array(tmp))\n",
    "        predicted_results.append(np.array(tmp))\n",
    "\n",
    "        if '.tif' in img_name:\n",
    "            target_path = os.path.join(gr_labels, img_name.replace('.tif', '.txt'))\n",
    "        else:\n",
    "            target_path = os.path.join(gr_labels, img_name.replace('.png', '.txt'))\n",
    "        target_results.append(np.loadtxt(target_path, ndmin=2))\n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # ax.imshow(gr_img)\n",
    "        # for bbox in target_results[img_id]:\n",
    "        #     classvalue, x1, y1, x2, y2, x3, y3, x4, y4 = bbox*gr_img.size[0]\n",
    "        #     ax.plot([x1, x2, x3, x4, x1], [y1, y2, y3, y4, y1], color = 'green')\n",
    "        # for bbox in predicted_results[img_id]:\n",
    "        #     x1, y1, x2, y2, x3, y3, x4, y4 = bbox*gr_img.size[0]\n",
    "        #     ax.plot([x1, x2, x3, x4, x1], [y1, y2, y3, y4, y1], color = 'red')\n",
    "\n",
    "    return predicted_results, target_results, image_path_with_detected_objects, total_execution_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch wise processing of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region: /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data/uttar_pradesh_most_15/images/, user_message: [refer] Where are the factories in the image? Give its oriented bounding box\n",
      "conf_threshold = 0.25, iou_threshold = 0.1\n",
      "Precision: 0.1037037037037037, Recall: 0.05303030303030303, F1 Score: 0.07017543859649124\n",
      "Total execution time(s): 96.04103755950928\n",
      "            predicted kilns  predicted_bg\n",
      "true kilns             14.0         250.0\n",
      "true_bg               121.0           0.0\n",
      "---------------------------------------------------------------\n",
      "region: /home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data/west_bengal_most_15/images/, user_message: [refer] Where are the factories in the image? Give its oriented bounding box\n",
      "conf_threshold = 0.25, iou_threshold = 0.1\n",
      "Precision: 0.13178294573643412, Recall: 0.08808290155440414, F1 Score: 0.10559006211180125\n",
      "Total execution time(s): 96.33750653266907\n",
      "            predicted kilns  predicted_bg\n",
      "true kilns             17.0         176.0\n",
      "true_bg               112.0           0.0\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "patch_size = 320\n",
    "overlap = 20 # 640x640\n",
    "# overlap = 75 # 2560x2560\n",
    "base_path = '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data'\n",
    "\n",
    "regions = [\n",
    "            # f'{base_path}/lucknow_airshed_most_15/images', \n",
    "         #   '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/swinir_data/lucknow_airshed_most_15/images',\n",
    "         #   '/home/shataxi.dubey/shataxi_work/vlm_on_planet/lucknow_high_resolution_zoom17',\n",
    "        #    '/home/shataxi.dubey/shataxi_work/vlm_on_planet/lucknow_high_resolution_zoom18',\n",
    "            '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data/uttar_pradesh_most_15/images/',\n",
    "         #    f'{base_path}/uttar_pradesh_most_15/swinir_images',\n",
    "         #   '/home/shataxi.dubey/shataxi_work/vlm_on_planet/uttar_pradesh_high_resolution_zoom17',\n",
    "        #    '/home/shataxi.dubey/shataxi_work/vlm_on_planet/uttar_pradesh_high_resolution_zoom18',\n",
    "           '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data/west_bengal_most_15/images/',\n",
    "         #   f'{base_path}/west_bengal_most_15/swinir_images',\n",
    "         #   '/home/shataxi.dubey/shataxi_work/vlm_on_planet/west_bengal_high_resolution_zoom17',\n",
    "        #    '/home/shataxi.dubey/shataxi_work/vlm_on_planet/west_bengal_high_resolution_zoom18'\n",
    "        # '/home/shataxi.dubey/shataxi_work/vlm_on_planet/lucknow_kilns_zoom19',\n",
    "        # '/home/shataxi.dubey/shataxi_work/vlm_on_planet/uttar_pradesh_kilns_zoom19',\n",
    "        # '/home/shataxi.dubey/shataxi_work/vlm_on_planet/west_bengal_kilns_zoom19'\n",
    "        ]\n",
    "\n",
    "locations = [\n",
    "            #  'lucknow_airshed_most_15', \n",
    "            #  'lucknow_airshed_most_15', \n",
    "            #  'lucknow_airshed_most_15', \n",
    "            #  'lucknow_airshed_most_15',\n",
    "             'uttar_pradesh_most_15',\n",
    "            #  'uttar_pradesh_most_15',\n",
    "            #  'uttar_pradesh_most_15', \n",
    "            #  'uttar_pradesh_most_15', \n",
    "             'west_bengal_most_15',\n",
    "            #  'west_bengal_most_15', \n",
    "            #  'west_bengal_most_15', \n",
    "            #  'west_bengal_most_15',\n",
    "             ]\n",
    "type = [\n",
    "   #  'lucknow_airshed_most_15_planet',\n",
    "   #  'lucknow_airshed_most_15_swinir',\n",
    "   #  'lucknow_airshed_most_15_zoom17',\n",
    "   #  'lucknow_airshed_most_15_zoom18',\n",
    "    'uttar_pradesh_most_15_planet',\n",
    "   #  'uttar_pradesh_most_15_swinir',\n",
    "   #  'uttar_pradesh_most_15_zoom17',\n",
    "   #  'uttar_pradesh_most_15_zoom18',\n",
    "    'west_bengal_most_15_planet',\n",
    "   #  'west_bengal_most_15_swinir',\n",
    "   #  'west_bengal_most_15_zoom17',\n",
    "   #  'west_bengal_most_15_zoom18',\n",
    "   #  'lucknow_kilns_zoom19',\n",
    "   #  'uttar_pradesh_kilns_zoom19',\n",
    "   #  'west_bengal_kilns_zoom19'\n",
    "]\n",
    "\n",
    "for user_message in user_messages:\n",
    "    for region, location, type in zip(regions, locations, type):\n",
    "        print(f'region: {region}, user_message: {user_message}')\n",
    "        gr_imgs_path = region+'/*'\n",
    "        gr_labels = f'/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data/{location}/labels'\n",
    "\n",
    "        predicted_results, target_results, image_path_with_detected_objects, total_execution_time = get_predicted_target_bbox_patch(chat, CONV_VISION, gr_imgs_path, patch_size, overlap, gr_labels, type, user_message)\n",
    "        new_predicted_results = add_class_confidence(predicted_results)\n",
    "        new_target_results = modify_class(target_results)\n",
    "        plot_results(user_message, gr_imgs_path, new_predicted_results, new_target_results, image_path_with_detected_objects, region, True)\n",
    "        cm, df = calculate_confusion_matrix(new_predicted_results, new_target_results)\n",
    "        # # print(f'Precision: {cm.precision}, Recall: {cm.recall}, F1 Score: {cm.f1_score}')\n",
    "        precision, recall, f1_score = calculate_precision_recall(df, new_predicted_results, new_target_results)\n",
    "        print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}')\n",
    "        print(f'Total execution time(s): {total_execution_time}')\n",
    "        print(df)\n",
    "\n",
    "        print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customise the Plot results according to IoU threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_results_based_on_iou(user_message, gr_imgs_path, new_predicted_results, new_target_results, iou_threshold, image_path_with_detected_objects, region, plot=False):\n",
    "    if not plot:\n",
    "        return\n",
    "    # n = len(image_path_with_detected_objects)\n",
    "    gr_img_path = sorted(glob(gr_imgs_path))\n",
    "    n = len(gr_img_path)\n",
    "    fig, ax = plt.subplots(nrows = n, ncols = 1 ,figsize=(120, 120))\n",
    "    ax = ax.flatten()\n",
    "    for i in range(n):\n",
    "        # img = Image.open(image_path_with_detected_objects[i]).convert('RGB') # predicted image\n",
    "        img = Image.open(gr_img_path[i]).convert('RGB') # original image\n",
    "        w, h = img.size\n",
    "        ax[i].imshow(img) \n",
    "        for bbox in new_target_results[i]:\n",
    "            classvalue, x1, y1, x2, y2, x3, y3, x4, y4 = bbox*w \n",
    "            ax[i].plot([x1, x2, x3, x4, x1], [y1, y2, y3, y4, y1], color = 'green')\n",
    "        for bbox in new_predicted_results[i]:\n",
    "            classvalue, x1, y1, x2, y2, x3, y3, x4, y4, conf = bbox*w \n",
    "            ax[i].plot([x1, x2, x3, x4, x1], [y1, y2, y3, y4, y1], color = 'red')\n",
    "        ax[i].set_axis_off()\n",
    "\n",
    "    fig.suptitle(f'IoU:{iou_threshold}_{user_message}')\n",
    "    region = region.replace('/','_')\n",
    "    plt.savefig(f'geochat_output_refer_{region}.png')\n",
    "    plt.close() # will not display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         105.           0.23701341]\n",
      " [  1.          94.           0.14710886]\n",
      " [  2.          80.           0.15956379]\n",
      " [  4.          72.           0.23209519]\n",
      " [  5.          61.           0.21893226]\n",
      " [  6.          59.           0.1458968 ]\n",
      " [  8.          46.           0.16139825]\n",
      " [ 10.          29.           0.21680819]\n",
      " [ 11.           2.           0.37664996]]\n",
      "9\n",
      "[[ 0.         97.          0.12260881]\n",
      " [ 2.         90.          0.21227805]\n",
      " [ 3.         87.          0.16453006]\n",
      " [ 4.         86.          0.24101132]\n",
      " [ 5.         85.          0.27092224]\n",
      " [ 8.         32.          0.37253849]\n",
      " [ 9.         28.          0.15158679]]\n",
      "7\n",
      "[[ 0.         76.          0.20611791]\n",
      " [ 1.         46.          0.41013578]\n",
      " [ 2.         32.          0.70055344]\n",
      " [ 4.         22.          0.15135311]\n",
      " [ 6.         14.          0.34886974]\n",
      " [ 7.         19.          0.23136314]]\n",
      "6\n",
      "[[ 1.         78.          0.19502506]\n",
      " [ 2.         91.          0.10748007]\n",
      " [ 5.         68.          0.10826061]]\n",
      "3\n",
      "[[  0.         105.           0.13270832]\n",
      " [  1.          94.           0.19331799]\n",
      " [  2.          97.           0.21069405]\n",
      " [  5.          72.           0.20787459]\n",
      " [  6.          60.           0.1495449 ]\n",
      " [  8.          53.           0.16897524]\n",
      " [ 10.          30.           0.13473089]]\n",
      "7\n",
      "[[0.00000000e+00 9.40000000e+01 1.51921194e-01]\n",
      " [2.00000000e+00 1.06000000e+02 1.55438305e-01]\n",
      " [3.00000000e+00 7.40000000e+01 1.02880135e-01]\n",
      " [7.00000000e+00 4.60000000e+01 1.92287124e-01]\n",
      " [8.00000000e+00 3.30000000e+01 1.19484241e-01]]\n",
      "5\n",
      "[[ 2.         66.          0.15589662]\n",
      " [ 4.         41.          0.37827902]\n",
      " [ 5.         29.          0.17822436]\n",
      " [ 8.          9.          0.44249071]]\n",
      "4\n",
      "[[  0.         107.           0.30069453]\n",
      " [  1.          82.           0.16537281]\n",
      " [  2.          96.           0.41579214]\n",
      " [  3.          78.           0.3030706 ]\n",
      " [  4.          58.           0.25984401]\n",
      " [  5.          61.           0.22883548]\n",
      " [  7.          41.           0.20865019]\n",
      " [  8.          28.           0.26756191]\n",
      " [  9.          25.           0.18295673]\n",
      " [ 10.           2.           0.26820163]]\n",
      "10\n",
      "[[ 0.         90.          0.44521689]\n",
      " [ 1.         91.          0.30055359]\n",
      " [ 2.         89.          0.21713983]\n",
      " [ 7.         50.          0.12282865]\n",
      " [ 8.         28.          0.28282754]\n",
      " [ 9.          6.          0.2012856 ]]\n",
      "6\n",
      "[[ 0.         91.          0.32069536]\n",
      " [ 1.         82.          0.35655821]\n",
      " [ 2.         49.          0.17225916]\n",
      " [ 3.         50.          0.23043725]\n",
      " [ 5.         12.          0.1191929 ]\n",
      " [ 7.         75.          0.20937091]\n",
      " [ 8.         31.          0.16993797]]\n",
      "7\n",
      "[[  0.         103.           0.11623553]\n",
      " [  2.          68.           0.13719917]\n",
      " [  3.          70.           0.25646612]\n",
      " [  4.          71.           0.19452686]\n",
      " [  5.          72.           0.1414737 ]\n",
      " [  9.          21.           0.15856524]]\n",
      "6\n",
      "[[  0.         103.           0.10588735]\n",
      " [  3.          92.           0.15093716]\n",
      " [  4.          46.           0.19563726]\n",
      " [  8.         100.           0.12996615]]\n",
      "4\n",
      "[[  0.         102.           0.16100808]\n",
      " [  1.          91.           0.163367  ]\n",
      " [  2.          93.           0.15560792]\n",
      " [  3.         104.           0.19833771]\n",
      " [  6.          75.           0.11051131]\n",
      " [  7.          50.           0.22774566]\n",
      " [  9.          57.           0.15330156]]\n",
      "7\n",
      "[[ 1.         87.          0.11435444]\n",
      " [ 4.         82.          0.19003763]\n",
      " [ 6.         73.          0.19042466]\n",
      " [ 7.         60.          0.2496175 ]\n",
      " [ 9.         37.          0.15351159]]\n",
      "5\n",
      "[[ 1.         97.          0.10469035]\n",
      " [ 2.         90.          0.19351219]\n",
      " [ 3.         95.          0.13393369]\n",
      " [ 4.         76.          0.15902777]\n",
      " [ 6.         71.          0.1197132 ]\n",
      " [ 7.         59.          0.29175183]\n",
      " [ 8.         57.          0.10649418]\n",
      " [ 9.         61.          0.25497449]\n",
      " [11.          9.          0.10890151]]\n",
      "9\n",
      "are total predictions same as total images 15\n"
     ]
    }
   ],
   "source": [
    "region = 'lucknow_zoom17_prompt1'\n",
    "# gr_imgs_path = '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/swinir_data/lucknow_airshed_most_15/images/*'\n",
    "gr_imgs_path = '/home/shataxi.dubey/shataxi_work/vlm_on_planet/lucknow_high_resolution_zoom17/*'\n",
    "# gr_imgs_path = '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data/uttar_pradesh_most_15/swinir_images/*'\n",
    "predicted_labels_path = '/home/shataxi.dubey/shataxi_work/vlm_on_planet/lucknow_airshed_most_15_zoom17/prompt1/*'\n",
    "ground_truth_labels_path = '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data/lucknow_airshed_most_15/labels/*'\n",
    "user_message = '[refer] Where are the brick kilns with chimney in the image? Give its oriented bounding box'\n",
    "iou_threshold = 0.1\n",
    "\n",
    "labels = sorted(glob(predicted_labels_path))\n",
    "new_predicted_results = []\n",
    "for label in labels:\n",
    "    boxes = np.loadtxt(label, ndmin = 2)\n",
    "    new_predicted_results.append(boxes)\n",
    "\n",
    "labels = sorted(glob(ground_truth_labels_path))\n",
    "new_target_results = []\n",
    "for label in labels:\n",
    "    boxes = np.loadtxt(label, ndmin = 2)\n",
    "    new_target_results.append(boxes)\n",
    "\n",
    "new_predicted_results = add_class_confidence(new_predicted_results)\n",
    "new_target_results = modify_class(new_target_results)\n",
    "\n",
    "filtered_predictions = []\n",
    "for targets, predictions in zip(new_target_results, new_predicted_results):\n",
    "    num_predictions, num_iou_levels = predictions.shape[0], 1\n",
    "    targets = targets*500\n",
    "    predictions = predictions*500\n",
    "    # print(f'total target {len(targets)}, total predictions {len(predictions)}')\n",
    "    iou = obb_iou(targets[:, 1:9].reshape(-1,4,2), predictions[:, 1:9].reshape(-1,4,2)) # (n x m) n targets and m predictions\n",
    "    matched_indices = np.where((iou >= iou_threshold)) # gives the indices of matched targets and predictions (2, num of matched predictions)\n",
    "    if matched_indices[0].shape[0]:\n",
    "        combined_indices = np.stack(matched_indices, axis=1) # gives the array of (matched target idx, matched predicted idx)\n",
    "        iou_values = iou[matched_indices][:, None] # gives the iou values of matched targets and predictions\n",
    "        matches = np.hstack([combined_indices, iou_values]) # horizontal stack of matched target idx, matched predicted idx and iou values\n",
    "\n",
    "        if matched_indices[0].shape[0] > 1:\n",
    "            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 1], return_index=True)[1]] # find the unique targets with which prediction is matched\n",
    "            matches = matches[np.unique(matches[:, 0], return_index=True)[1]] # find the unique prediction with which target is matched\n",
    "\n",
    "        print(matches)\n",
    "        print(len(matches))\n",
    "        predictions = predictions[matches[:, 1].astype(int)]\n",
    "        filtered_predictions.append(predictions/500)\n",
    "    else:\n",
    "        filtered_predictions.append(np.array([]))\n",
    "\n",
    "print('are total predictions same as total images',len(filtered_predictions))\n",
    "plot_results_based_on_iou(user_message, gr_imgs_path, filtered_predictions, new_target_results, iou_threshold, [], region, plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geochat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
